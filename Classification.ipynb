{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Classifying Cookies from Non-Cookies\n",
    "Combine the existing chocolate chip cookie dataset with xxx."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cookie_images_dir = os.path.join('./dataset/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Build a custom cookie/non-cookie dataset. Combines the cookie images with CIFAR-10 - a Keras dataset with 10 categories and 50,000 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_bin_class_dataset(cookie_images_dir, num_non_cookie_samples=1000, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Create a binary classification dataset of cookies vs non-cookies\n",
    "    \n",
    "    Parameters:\n",
    "    cookie_dir: Path to directory containing cookie images\n",
    "    num_non_cookie_samples: Number of non-cookie images to include\n",
    "    target_size: Target size for all images\n",
    "    \n",
    "    Returns:\n",
    "    (x_train, y_train), (x_test, y_test): Training and test datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and preprocess cookie images\n",
    "    cookie_images_path = pathlib.Path(cookie_images_dir)\n",
    "    cookie_images = list(cookie_images_path.glob('*.[jJ][pP][gG]')) + list(cookie_images_path.glob('*.[pP][nN][gG]'))\n",
    "    \n",
    "    # Load and preprocess cookie images\n",
    "    cookies = []\n",
    "    for img_path in cookie_images:\n",
    "        img = image.load_img(img_path, target_size=target_size)\n",
    "        img_array = image.img_to_array(img)\n",
    "        cookies.append(img_array)\n",
    "    \n",
    "    cookies = np.array(cookies)\n",
    "    \n",
    "    # Load CIFAR-10 as source of non-cookie images\n",
    "    (cifar_x_train, cifar_y_train), (cifar_x_test, cifar_y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "    \n",
    "    # Resize CIFAR images to match cookie image size\n",
    "    non_cookies = []\n",
    "    for img in cifar_x_train[:num_non_cookie_samples]:\n",
    "        resized = tf.image.resize(img, target_size)\n",
    "        non_cookies.append(resized)\n",
    "    \n",
    "    non_cookies = np.array(non_cookies)\n",
    "    \n",
    "    # Combine datasets\n",
    "    X = np.concatenate([cookies, non_cookies])\n",
    "    y = np.concatenate([np.ones(len(cookies)), np.zeros(len(non_cookies))])\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    \n",
    "    # Split into train and test sets\n",
    "    split = int(0.8 * len(X))\n",
    "    x_train, x_test = X[:split], X[split:]\n",
    "    y_train, y_test = y[:split], y[split:]\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "# Data augmentation for training\n",
    "def create_data_generators(x_train, y_train, x_test, y_test, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create data generators with augmentation for training\n",
    "    \"\"\"\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "    \n",
    "    test_datagen = ImageDataGenerator()\n",
    "    \n",
    "    train_generator = train_datagen.flow(\n",
    "        x_train, y_train,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    test_generator = test_datagen.flow(\n",
    "        x_test, y_test,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    return train_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Load the dataset and create generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the dataset\n",
    "(x_train, y_train), (x_test, y_test) = create_bin_class_dataset(cookie_images_dir)\n",
    "\n",
    "# Create data generators\n",
    "train_generator, test_generator = create_data_generators(x_train, y_train, x_test, y_test)\n",
    "\n",
    "# Print dataset info\n",
    "print(f\"Training samples: {len(x_train)}\")\n",
    "print(f\"Test samples: {len(x_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Build a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization, Input, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "def build_cookie_classifier(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Build a CNN model for cookie classification with better uncertainty handling\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape: Shape of input images (height, width, channels)\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled Keras model\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        # Global Average Pooling\n",
    "        GlobalAveragePooling2D(),\n",
    "    \n",
    "        # Dense Layers\n",
    "        # Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(2, activation='softmax')  # Changed to 2 units with softmax instead of Binary classification\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']#, tf.keras.metrics.AUC()]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model, train_generator, test_generator, epochs=50):\n",
    "    \"\"\"\n",
    "    Train the model with callbacks for early stopping and learning rate reduction\n",
    "    \n",
    "    Parameters:\n",
    "    model: Compiled Keras model\n",
    "    train_generator: Training data generator\n",
    "    test_generator: Validation data generator\n",
    "    epochs: Maximum number of epochs to train\n",
    "    \n",
    "    Returns:\n",
    "    history: Training history\n",
    "    \"\"\"\n",
    "    # Callbacks\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=5,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=epochs,\n",
    "        validation_data=test_generator,\n",
    "        callbacks=[early_stopping, reduce_lr]\n",
    "    )\n",
    "    \n",
    "    return history\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training history including accuracy and loss curves\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # Accuracy plot\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "You can start training for 10 epochs -- this may take a few minutes to run. Increasing the number of epochs reduces the model loss.\n",
    "\n",
    "| Epochs           | Accuracy    | Loss    |\n",
    "|------------------|-------------|---------|\n",
    "| 20               | 0.9994      | 0.0029  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and compile the model\n",
    "model = build_cookie_classifier()\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, train_generator, test_generator, epochs=20)\n",
    "\n",
    "# Plot training history\n",
    "plot_training_history(history)\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(f\"\\nTest Loss: {test_loss:.4f}\")\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Predict images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(model, image_path, confidence_threshold=0.8, target_size=(224, 224)):\n",
    "    \"\"\"\n",
    "    Make a prediction on a single image with uncertainty handling\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained Keras model\n",
    "    image_path: Path to the image file\n",
    "    confidence_threshold: Minimum confidence required for a definitive prediction\n",
    "    target_size: Size to resize the image to\n",
    "    \n",
    "    Returns:\n",
    "    prediction: Class prediction and confidence\n",
    "    \"\"\"\n",
    "    # Load and preprocess the image\n",
    "    img = image.load_img(image_path, target_size=target_size)\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "    img_array = img_array.astype('float32') / 255.0  # Normalize\n",
    "    \n",
    "    # Make prediction\n",
    "    predictions = model.predict(img_array)[0]\n",
    "    max_confidence = np.max(predictions)\n",
    "    predicted_class = np.argmax(predictions)\n",
    " \n",
    "    # Determine prediction based on confidence threshold\n",
    "    if max_confidence >= confidence_threshold:\n",
    "        if predicted_class == 1:\n",
    "            prediction_text = \"Cookie\"\n",
    "        else:\n",
    "            prediction_text = \"Not Cookie\"\n",
    "    else:\n",
    "        prediction_text = \"Uncertain\"\n",
    "        \n",
    "    # Display the image with prediction\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.title(f'Prediction: {prediction_text}\\nConfidence: {max_confidence:.2%}')\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'prediction': prediction_text,\n",
    "        'confidence': max_confidence,\n",
    "        'probabilities': {\n",
    "            'not_cookie': predictions[0],\n",
    "            'cookie': predictions[1]\n",
    "        }\n",
    "    }\n",
    "\n",
    "def print_result(result):\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(f\"Class: {result['prediction']}\")\n",
    "    print(f\"Confidence: {result['confidence']:.2%}\")\n",
    "    print(\"\\nClass Probabilities:\")\n",
    "    print(f\"Not Cookie: {result['probabilities']['not_cookie']:.2%}\")\n",
    "    print(f\"Cookie: {result['probabilities']['cookie']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on new image\n",
    "result = predict_image(model, \"./prediction.jpg\")\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on new image\n",
    "result = predict_image(model, \"./non-cookie 1.jpg\")\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on new image\n",
    "result = predict_image(model, \"./non-cookie 2.png\")\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction on new image\n",
    "result = predict_image(model, \"./non-cookie 3.jpg\")\n",
    "print_result(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Export\n",
    "\n",
    "Export the model to a keras file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('classification-model.keras')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
