{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa10b9d8-e4bd-48a6-988a-ddb7939ccb32",
   "metadata": {},
   "source": [
    "# Counting Chocolate Chips in Chocolate Chip Cookies\n",
    "Explore the dataset. It contains training cookies, and labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cf6ea-81c8-496e-bb07-fb0a50ce3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "training_images_dir = os.path.join('./dataset/images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53910275-fc51-4ae0-ae5d-4bc33a0ef275",
   "metadata": {},
   "source": [
    "The cookies are images. Let's see what the first ten filenames look like, and display to total number of images in that directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b016c-3c67-4ad5-8b16-16af14925b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Examples: ', os.listdir(training_images_dir)[:10])\n",
    "print('Total number of training images:', len(os.listdir(training_images_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451fbc38-0d31-4cc8-9a65-aad9cdd9b9dc",
   "metadata": {},
   "source": [
    "Display one of the cookies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661ee9d-ef63-4465-b489-96b61e5c3cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import load_img\n",
    "\n",
    "chunky_cookie_img = load_img('./dataset/images/chunky 28 34.jpg')\n",
    "\n",
    "plt.imshow(chunky_cookie_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41837c-9b35-45cb-a615-02c5cb51a02a",
   "metadata": {},
   "source": [
    "The labels are numbers. The label indicates the number of visible chocolate chips in that cookie. Indicating the correct number of chocolate chips in a cookie is difficult for a number of reasons:\n",
    "* some chocolate chips are only `partially` visible.\n",
    "* some chocolate chips are `divided` by dough and appear as two individual chocolate chips.\n",
    "* sometimes chocolate chips `smear` off on other cookies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb3fb3c-ef7f-47ee-899e-cf774a40107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "training_labels_file = './dataset/labels/labels.csv'\n",
    "training_labels_data_frame = pd.read_csv(training_labels_file)\n",
    "\n",
    "# Print the first 10 labels\n",
    "print('Examples: ', training_labels_data_frame['label'].head(10).to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4381871-038b-4811-bf9e-42a14a5dddee",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "The ultimate concept is that they narrow down the content of the image to focus on specific parts and this will likely improve the model accuracy. \n",
    "\n",
    "If you've ever done image processing using a filter (like [this](https://en.wikipedia.org/wiki/Kernel_(image_processing))), then convolutions will look very familiar. In short, you take an array (usually 3x3 or 5x5) and scan it over the entire image. By changing the underlying pixels based on the formula within that matrix, you can do things like edge detection. So, for example, if you look at the above link, you'll see a 3x3 matrix that is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has the edges enhanced.\n",
    "\n",
    "This is perfect for computer vision because it often highlights features that distinguish one item from another. Moreover, the amount of information needed is then much less because you'll just train on the highlighted features.\n",
    "\n",
    "That's the concept of **Convolutional Neural Networks**. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers is more focused and possibly more accurate.\n",
    "\n",
    "Run the code below. This is the same neural network as earlier, but this time with [Convolution](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPooling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers added first. It will take longer, but look at the impact on the accuracy.\n",
    "\n",
    "Look at the code again, and see, step by step how the convolutions were built. Instead of the input layer at the top, you added a [Conv2D layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D). The parameters are:\n",
    "\n",
    "1. The number of convolutions you want to generate. The value here is purely arbitrary but it's good to use powers of 2 starting from 32.\n",
    "2. The size of the Convolution. In this case, a 3x3 grid.\n",
    "3. The activation function to use. In this case, you used a ReLU, which you might recall is the equivalent of returning `x` when `x>0`, else return `0`.\n",
    "4. In the first layer, the shape of the input data.\n",
    "\n",
    "You'll follow the convolution with a [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layer which is designed to compress the image, while maintaining the content of the features that were highlighted by the convolution. By specifying `(2,2)` for the MaxPooling, the effect is to quarter the size of the image. Without going into too much detail here, the idea is that it creates a 2x2 array of pixels, and picks the biggest one. Thus, it turns 4 pixels into 1. It repeats this across the image, and in doing so, it halves both the number of horizontal and vertical pixels, effectively reducing the image to 25% of the original image.\n",
    "\n",
    "We feed the images into the convolutions, and flatten the final result to feed into the densely connected layers. Note that we count the number of chocolate chips, i.e. the final output is a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2febbbc8-4448-445c-a3cf-f513ad3a43ad",
   "metadata": {},
   "source": [
    "### What is the optimal number of convolutional layers?\n",
    "We have tried with three, four or five convolutional layers. Four layers produces the best result.\n",
    "\n",
    "| Number of layers | Loss | Mean abs. err. |\n",
    "|------------------|------|----------------|\n",
    "| 3                | 7.2  | 2.1            |\n",
    "| 4                | 5.0  | 1.8            |\n",
    "| 5                | 9.0  | 2.5            |\n",
    "\n",
    "CNN vs. MobileNetV2.\n",
    "\n",
    "| Number of layers | Loss | Mean abs. err. |\n",
    "|------------------|------|----------------|\n",
    "| CNN              | 0.7  | 0.7            |\n",
    "| MobileNetV2      | 1.4  | 0.9            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810a523a-8472-4792-b98a-969c5f68be1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# base_model = MobileNetV2(\n",
    "#     input_shape=(300, 300, 3), \n",
    "#     include_top=False, \n",
    "#     weights='imagenet'\n",
    "# )\n",
    "# base_model.trainable = False\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    # base_model,\n",
    "    # tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    # Note the input shape is the desired size of the image 300x300 with 3 bytes color\n",
    "    tf.keras.layers.Input(shape=(300, 300, 3)),\n",
    "    # This is the first convolution\n",
    "    tf.keras.layers.Conv2D(16, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    # The second convolution\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The third convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # The fourth convolution\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    # Single output for the count of chocolate chips\n",
    "    tf.keras.layers.Dense(1) \n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5402dc-70d1-41b9-a8d2-60f613025320",
   "metadata": {},
   "source": [
    "| Dropout       | Loss | Mean abs. err. |\n",
    "|---------------|------|----------------|\n",
    "| 0.2           | 1.2  | 0.8            |\n",
    "| 0.3           | 1.3  | 0.9            |\n",
    "| 0.5           | 1.0  | 0.8            |\n",
    "| 0.6           | 2.4  | 1.3            |\n",
    "\n",
    "You can review the network architecture and the output shapes with model.summary()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac6102-2372-43bd-834a-9d0e6698c1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d069f2-cc45-40bc-9a6f-02791f6a8218",
   "metadata": {},
   "source": [
    "To build a model in TensorFlow that takes images of cookies as inputs and counts the number of chocolate chips in them, you can use a Convolutional Neural Network (CNN). This type of network is well-suited for image processing tasks. The output layer of the model should be a single neuron since we're predicting a single scalar value (the count of chocolate chips).\n",
    "\n",
    "We will train the model with the [`mean_squared_error`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/MeanSquaredError) loss because we measure absolute loss.  (For a refresher on loss metrics, see this [Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/descending-into-ml/video-lecture).) You will use the [`adam`](https://wikipedia.org/wiki/Stochastic_gradient_descent#Adam) optimizer. A sophisticated form of SGD, it automatically adapts the learning rate during training. You will want to monitor the mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef93c17-08d4-44a9-8aa3-8566c08d5fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',  # Adam optimizer\n",
    "    loss='mean_squared_error',  # Mean squared error for regression\n",
    "    metrics=['mean_absolute_error'])  # Mean absolute error as a metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5318e3-1aa7-43b0-9540-6b72d440517f",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Next step is to set up the data generators that will read pictures in the source folders, convert them to `float32` tensors, and feed them (with their labels) to the model. You'll have one generator for the training images and one for the validation images. These generators will yield batches of images of size 300x300 and their labels (binary).\n",
    "\n",
    "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network (i.e. It is uncommon to feed raw pixels into a ConvNet.) In this case, you will preprocess the images by normalizing the pixel values to be in the `[0, 1]` range (originally all values are in the `[0, 255]` range).\n",
    "\n",
    "In Keras, this can be done via the `keras.preprocessing.image.ImageDataGenerator` class using the `rescale` parameter. This `ImageDataGenerator` class allows you to instantiate generators of augmented image batches (and their labels) via `.flow(data, labels)`, `.flow_from_directory(directory)`, or `.flow_from_dataframe(df)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b6b994-c6b2-4ee6-916b-d2bf9569425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_dataset(images_dir, labels_file, target_size=(300, 300), batch_size=10):\n",
    "    # Read the CSV file into a DataFrame\n",
    "    df = pd.read_csv(labels_file)\n",
    "    \n",
    "    # Create an ImageDataGenerator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2  # 20% of data for validation\n",
    "    )\n",
    "\n",
    "    # Create the train and validation generators\n",
    "    training_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=images_dir,\n",
    "        x_col='filename',\n",
    "        y_col='label',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',  # Use 'raw' to get labels as they are in the CSV\n",
    "        subset='training'\n",
    "    )\n",
    "\n",
    "    validation_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df,\n",
    "        directory=images_dir,\n",
    "        x_col='filename',\n",
    "        y_col='label',\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='raw',  # Use 'raw' to get labels as they are in the CSV\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    return training_generator, validation_generator\n",
    "\n",
    "# Create generators\n",
    "training_generator, validation_generator = load_dataset(training_images_dir, training_labels_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b874c3ed-b969-4c42-b294-80e20c84a5cc",
   "metadata": {},
   "source": [
    "\n",
    "| Batch size       | Loss | Mean abs. err. |\n",
    "|------------------|------|----------------|\n",
    "| 10               | 1.1  | 0.8            |\n",
    "| 8                | 5.0  | 1.8            |\n",
    "| 5                | 5.3  | 1.8            |\n",
    "| 2                | 2.4  | 1.3            |\n",
    "\n",
    "Some batch sizes work better than others. It's a little unclear why. Smaller batch sizes probably over-fit.\n",
    "\n",
    "| # of neurons     | Loss | Mean abs. err. |\n",
    "|------------------|------|----------------|\n",
    "| 128 (fastest)    | 9.1  | 2.2            |\n",
    "| 256              | 6.8  | 2.1            |\n",
    "| 512              | 4.4  | 1.7            |\n",
    "| 1024 (slowest)   | 4.5  | 1.6            |\n",
    "\n",
    "The number of neurons in the hidden layer impacts training speed and accuracy. Diminishing returns, and possibly over-fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566ac1fc-1f98-449b-948b-39d40c34b171",
   "metadata": {},
   "source": [
    "## Creating a Callback class\n",
    "\n",
    "You can create a callback by defining a class that inherits the [`tf.keras.callbacks.Callback`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback) base class. From there, you can define available methods to set where the callback will be executed. For instance below, you will use the [`on_epoch_end()`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback#on_epoch_end) method to check the loss at each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7625635-d61c-4ab1-bbaf-df05c84e9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    '''\n",
    "    Halts the training when the loss falls below 0.8\n",
    "\n",
    "    Args:\n",
    "      epoch (integer) - index of epoch (required but unused in the function definition below)\n",
    "      logs (dict) - metric results from the training epoch\n",
    "    '''\n",
    "\n",
    "    # Check the loss\n",
    "    if(logs.get('loss') < 0.8):\n",
    "\n",
    "      # Stop if threshold is met\n",
    "      print(\"\\nLoss is lower than 0.8 so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "# Instantiate class\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3187d00a-44a5-4998-b74d-273c0e0d94e1",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "You can start training for 15 epochs -- this may take a few minutes to run. Increasing the number of epochs reduces the model loss.\n",
    "\n",
    "| Epochs           | Loss | Mean abs. err. |\n",
    "|------------------|------|----------------|\n",
    "| 10               | 6.1  | 2.1            |\n",
    "| 15               | 4.6  | 1.6            |\n",
    "| 20               | 2.7  | 1.2            |\n",
    "| 50               | <0.8 | <0.7           |\n",
    "| 100              | <0.7 | <0.6           |\n",
    "| 1000             | <0.5 | <0.6           |\n",
    "\n",
    "Do note the values per epoch.\n",
    "\n",
    "The `loss` and `mean_absolute_error` are great indicators of progress in training. `loss` measures the current model prediction against the known labels, calculating the result. `mean_absolute_error`, on the other hand, is the deviation from the correct number of chocolate chips.\n",
    "\n",
    "`model.fit` seems to remember what has been trained, so unless you re-define and re-compile the model it continues to train the same model. Cumulatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b30cd2-1e7f-4aef-826c-5430f91360c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "      training_generator,\n",
    "      validation_data=validation_generator,\n",
    "      epochs=10, #50,\n",
    "      verbose=1,\n",
    "      callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bb5a6-fec4-4889-90c6-45044ebe79eb",
   "metadata": {},
   "source": [
    "# Visualizing the Convolutions and Pooling\n",
    "\n",
    "Let's explore how to show the convolutions graphically. The cell below prints the first 100 labels in the test set, and you can see that the ones at index `0`, index `23` and index `28` are all the same value (i.e. `9`). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the dense layer is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a0a35-2e97-4a05-a1a9-2cf2081d3ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# from tensorflow.keras import models\n",
    "\n",
    "\n",
    "# Optionally, call the model with some dummy data\n",
    "# dummy_input = tf.random.normal((1, 300, 300, 3))  # Shape should match your input shape\n",
    "# model(dummy_input)  # This will make the model see the input shape\n",
    "\n",
    "f, axarr = plt.subplots(2, 4, figsize=(20, 5))\n",
    "\n",
    "# Get the first batch of images and labels\n",
    "training_images, training_labels = next(training_generator)\n",
    "\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=3\n",
    "CONVOLUTION_NUMBER = 7\n",
    "\n",
    "# Get only the convolutional layer outputs (we don't need pooling, flatten, dense, etc.)\n",
    "conv_layers = [layer.output for layer in model.layers if 'conv2d' in layer.name]\n",
    "\n",
    "# Create the activation model with only conv layers\n",
    "activation_model = tf.keras.models.Model(inputs=model.inputs, outputs=conv_layers)\n",
    "\n",
    "# # Get activations\n",
    "# activations = activation_model.predict(training_image)\n",
    "\n",
    "# # Plot activations for each conv layer\n",
    "# for i, activation in enumerate(activations):\n",
    "#     row = i // 4\n",
    "#     col = i % 4\n",
    "#     axarr[row, col].imshow(activation[0, :, :, 0], cmap='viridis')\n",
    "#     axarr[row, col].set_title(f'Conv2D_{i+4}')  # Adding 4 since your layers start at conv2d_4\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(training_images[FIRST_IMAGE].reshape(1, 300, 300, 3))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  \n",
    "  f2 = activation_model.predict(training_images[SECOND_IMAGE].reshape(1, 300, 300, 3))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4359cf-ef0c-45fe-86dd-0804c56a18c2",
   "metadata": {},
   "source": [
    "### Model Prediction\n",
    "\n",
    "Now take a look at actually running a prediction using the model. This code will allow you to choose 1 or more files from your file system, upload them, and run them through the model, giving an indication of whether the object is a horse or a human.\n",
    "\n",
    "_**Note:** Old versions of the Safari browser might have compatibility issues with the code block below. If you get an error after you select the images(s) to upload, you can consider updating your browser to the latest version. If not possible, please comment out or skip the code block below, uncomment the next code block and run it._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96262757-84b9-429f-b475-77a992af807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE: If you are using Safari and this cell throws an error,\n",
    "## please skip this block and run the next one instead.\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import load_img, img_to_array\n",
    "\n",
    "path = \"./prediction.jpg\"\n",
    "img = load_img(path, target_size=(300, 300))\n",
    "x = img_to_array(img)\n",
    "x /= 255\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = model.predict(images, batch_size=10)\n",
    "print(f\"chocolate chips: {classes[0]}\")\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6206532-ff47-4168-8574-be45593dfe33",
   "metadata": {},
   "source": [
    "### Export\n",
    "\n",
    "Export the model to an h5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5a8349-98b3-4723-b1be-db013d4d2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b95bc-32c1-4f4e-87a8-663a8fa5e2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
